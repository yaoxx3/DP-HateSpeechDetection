





import pandas as pd

df = pd.read_csv('white-supremacist-forum.csv')
df


# 4 kinds of labels exist
df['label'].value_counts()


# filter invalid label
correctLabels = ['noHate', 'hate']
df = df.loc[df['label'].isin(correctLabels)]
df['label'].value_counts()


df['text'].value_counts()


df





# Get descriptive summary for numerical features
summary_numeric = df.describe()

# Include categorical data in the summary
summary_all = df.describe(include='all')

print("Summary for Numerical Features:")
print(summary_numeric)

print("\nSummary for All Features (Numerical and Categorical):")
print(summary_all)


# Check for any null values in each column
null_values = df.isnull().any()

print("Columns with Null Values:")
print(null_values)


# label

from matplotlib import pyplot as plt
import seaborn as sns
df.groupby('label').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)





from transformers import BertTokenizer
from sklearn.model_selection import train_test_split

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
def encode_texts(texts):
    return tokenizer(texts, padding=True, truncation=True, max_length=128, return_tensors="pt")


from torch.utils.data import Dataset, DataLoader

class HateSpeechDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)
        return item

    def __len__(self):
        return len(self.labels)





from imblearn.over_sampling import RandomOverSampler
ros = RandomOverSampler(random_state=0)
train_texts_resampled, train_labels_resampled = ros.fit_resample(df['text'].to_frame(), df['label'].to_frame())


train_texts_resampled.info()


train_labels_resampled.value_counts()


train_texts_resampled = train_texts_resampled.squeeze()
train_labels_resampled = train_labels_resampled.squeeze()


from sklearn.model_selection import train_test_split

# Initial split to separate out the test set
train_val_texts, test_texts, train_val_labels, test_labels = train_test_split(
    train_texts_resampled, train_labels_resampled, test_size=0.1, random_state=42)

# Further split the training set into training and validation sets
train_texts, val_texts, train_labels, val_labels = train_test_split(
    train_val_texts, train_val_labels, test_size=0.1, random_state=42)


train_labels


import numpy as np
# Convert labels to numeric
label_mapping = {'noHate': 0, 'hate': 1}
train_labels = train_labels.map(label_mapping)
test_labels = test_labels.map(label_mapping)





from collections import Counter
from imblearn.under_sampling import AllKNN
allknn = AllKNN()
train_texts_res, train_labels_res = allknn.fit_resample(train_texts, train_labels)
print('raw dataset shape %s' % Counter(train_labels))
print('Resampled dataset shape %s' % Counter(train_labels_res))


# Preprocess texts
train_encodings = encode_texts(train_texts.tolist())
val_encodings = encode_texts(val_texts.tolist())
test_encodings = encode_texts(test_texts.tolist())


# Prepare datasets
train_dataset = HateSpeechDataset(train_encodings, train_labels.tolist())
val_dataset = HateSpeechDataset(val_encodings, val_labels.tolist())
test_dataset = HateSpeechDataset(test_encodings, test_labels.tolist())





from transformers import BertForSequenceClassification

model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)


from transformers import AdamW

optimizer = AdamW(model.parameters(), lr=5e-5)

# Define your training loop, including batch processing, optimization, and validation


import torch
from torch.utils.data import DataLoader
from sklearn.metrics import accuracy_score, classification_report

def evaluate(model, dataset, device='cpu'):
    model.eval()  # Set the model to evaluation mode
    predictions = []
    true_labels = []

    with torch.no_grad():
        for batch in DataLoader(dataset, batch_size=16):
            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}
            label = batch['labels'].to(device).to(torch.long)  # Ensure labels are long
            outputs = model(**inputs)
            logits = outputs.logits
            preds = torch.argmax(logits, dim=1)
            predictions.extend(preds.cpu().numpy())
            true_labels.extend(label.cpu().numpy())

    model.train() # turn back to training mode
    # Calculate and print metrics
    accuracy = accuracy_score(true_labels, predictions)
    report = classification_report(true_labels, predictions, target_names=['noHate', 'hate'])

    print(f"Accuracy: {accuracy}\n")
    print(report)

# Assuming you have a `device` variable set (e.g., to 'cuda' if using GPU)
evaluate(model, test_dataset)








# freeze all layers, except for the last encoder and above (BertPooler and Classifier)
trainable_layers = [model.bert.encoder.layer[-1], model.bert.pooler, model.classifier]
total_params = 0
trainable_params = 0

for p in model.parameters():
        p.requires_grad = False
        total_params += p.numel()

for layer in trainable_layers:
    for p in layer.parameters():
        p.requires_grad = True
        trainable_params += p.numel()


BATCH_SIZE = 32
MAX_PHYSICAL_BATCH_SIZE = 8


from torch.utils.data import DataLoader, RandomSampler, SequentialSampler
from opacus.utils.uniform_sampler import UniformWithReplacementSampler


train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE)
test_dataloader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=BATCH_SIZE)


EPOCHS = 3
LOGGING_INTERVAL = 500 # once every how many steps we run evaluation cycle and report metrics
EPSILON = 7.5
DELTA = 1 / len(train_dataloader) # Parameter for privacy accounting. Probability of not achieving privacy guarantees


from opacus import PrivacyEngine

MAX_GRAD_NORM = 0.1

privacy_engine = PrivacyEngine()
model, optimizer, train_dataloader = privacy_engine.make_private_with_epsilon(
    module=model,
    optimizer=optimizer,
    data_loader=train_dataloader,
    target_delta=DELTA,
    target_epsilon=EPSILON, 
    epochs=EPOCHS,
    max_grad_norm=MAX_GRAD_NORM,
    poisson_sampling=False
)


# Move the model to appropriate device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


from opacus.utils.batch_memory_manager import BatchMemoryManager
from tqdm.notebook import tqdm
import numpy as np

for epoch in range(1, EPOCHS+1):
    losses = []

    with BatchMemoryManager(
        data_loader=train_dataloader, 
        max_physical_batch_size=MAX_PHYSICAL_BATCH_SIZE, 
        optimizer=optimizer
    ) as memory_safe_data_loader:
        for step, batch in enumerate(tqdm(memory_safe_data_loader)):
            optimizer.zero_grad()
            
            batch = tuple(t.to(device) for t in batch.values())
            
            inputs = {'input_ids':      batch[0],
                    'attention_mask': batch[1],
                    'token_type_ids': batch[2],
                    'labels':         batch[3]}
            
            outputs = model(**inputs) # output = loss, logits, hidden_states, attentions

            loss = outputs[0]
            loss.backward()
            losses.append(loss.item())
            
            optimizer.step()

            if step > 0 and step % LOGGING_INTERVAL == 0:
                train_loss = np.mean(losses)
                eps = privacy_engine.get_epsilon(DELTA)

                evaluate(model, test_dataset)

                print(
                  f"Epoch: {epoch} | "
                  f"Step: {step} | "
                  f"Train loss: {train_loss:.3f} | "
                  f"É›: {eps:.2f}"
                )



