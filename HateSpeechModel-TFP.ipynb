{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddea9516-bdaf-4813-ae8b-e8bcf0dda529",
   "metadata": {},
   "source": [
    "# Differentially Private Hate Speech Detection - TensorFlow Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3caa6e-3c13-48e9-b570-438409f6364b",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "587d5617-205f-4f30-92e1-577e2fac232f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                               text   label\n",
      "0           0  As of March 13th , 2014 , the booklet had been...  noHate\n",
      "1           1  In order to help increase the booklets downloa...  noHate\n",
      "2           2  ( Simply copy and paste the following text int...  noHate\n",
      "3           3  Click below for a FREE download of a colorfull...    hate\n",
      "4           4  Click on the `` DOWNLOAD ( 7.42 MB ) '' green ...  noHate\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('white-supremacist-forum.csv')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "517f93eb-01bc-476f-8f25-6dd3d5de33d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "noHate    9507\n",
       "hate      1196\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter invalid label\n",
    "correctLabels = ['noHate', 'hate']\n",
    "df = df.loc[df['label'].isin(correctLabels)]\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8b3c5f-e9c1-41e7-af1d-f9368aca5b6e",
   "metadata": {},
   "source": [
    "The dataset is imbalanced(normal for hate speech corpus). Tried oversampling below, not working terribly well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3143801a-cef6-4a50-8f91-efaec2a64f52",
   "metadata": {},
   "source": [
    "## Experiment: Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d53a1189-4cfb-4cb5-bef2-f84508355577",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "train_texts_resampled, train_labels_resampled = ros.fit_resample(df['text'].to_frame(), df['label'].to_frame())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a6e149c7-e434-4e1a-ab38-2ad14c16ee1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts_resampled.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "26cc8ac1-c0c1-4869-b743-5ac4ce7bb6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_resampled.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5740556a-80ab-4c24-b1cf-71d375fbbb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts_resampled = train_texts_resampled.squeeze()\n",
    "train_labels_resampled = train_labels_resampled.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "266ce2f3-563e-49c8-8585-4c058e24f476",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Initial split to separate out the test set\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df['text'], df['label'], test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3b5b09-c017-404e-bac5-91c4103e525f",
   "metadata": {},
   "source": [
    "# Data Preparation for ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cb03c82b-f9d2-4953-beed-332a1b09f37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "def encode_texts(texts):\n",
    "    return tokenizer(texts, padding=True, truncation=True, max_length=38, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "86f4a34b-d50b-4ea8-abac-d11edf26bd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Convert labels to numeric\n",
    "label_mapping = {'noHate': 0, 'hate': 1}\n",
    "train_labels = train_labels.map(label_mapping)\n",
    "test_labels = test_labels.map(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "77b3473b-2b45-49a4-8efc-0679c774044a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1458     0\n",
       "10917    0\n",
       "10530    1\n",
       "583      0\n",
       "6087     0\n",
       "        ..\n",
       "5846     0\n",
       "5296     0\n",
       "5499     0\n",
       "867      0\n",
       "7418     0\n",
       "Name: label, Length: 9632, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7dc4633e-e614-4207-9029-9ffbb08503d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    953\n",
       "1    118\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7504c3b6-d5fc-41c7-81a5-0855edffca2b",
   "metadata": {},
   "source": [
    "## Experiment: Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "62ee4408-d8af-41bd-adcd-b5287143868f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# undersampling the dataset, noHate:hate=1:1\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "undersample = RandomUnderSampler(sampling_strategy=1)\n",
    "train_texts, train_labels = undersample.fit_resample(train_texts.to_frame(), train_labels.to_frame())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4f156f90-9221-4a3d-b379-c41602008381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1458     So people go out of the country for a few year...\n",
       "10917    Ive never liked rap , or to be closely associa...\n",
       "10530    It should come as no surprise that the Jews st...\n",
       "583      Well yes that s the thing there is a differenc...\n",
       "6087     I'm from France but it 's so hard to find any ...\n",
       "                               ...                        \n",
       "5846     For God 's sake nobody cares about this childi...\n",
       "5296     Let me know if White girls making out with bla...\n",
       "5499     im from wilbraham myself i 'm looking to organ...\n",
       "867      I would definitely not recommend becoming a te...\n",
       "7418                            He should sue for racism .\n",
       "Name: text, Length: 9632, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "31c711a2-57f1-4215-af37-26b06d24135a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1458     0\n",
       "10917    0\n",
       "10530    1\n",
       "583      0\n",
       "6087     0\n",
       "        ..\n",
       "5846     0\n",
       "5296     0\n",
       "5499     0\n",
       "867      0\n",
       "7418     0\n",
       "Name: label, Length: 9632, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c69e0d0-33a3-49d6-9f82-4eecd4187ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = train_texts.squeeze()\n",
    "train_labels = train_labels.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f80a6f01-bb33-4dff-9b7a-229df60fe9f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1458     So people go out of the country for a few year...\n",
       "10917    Ive never liked rap , or to be closely associa...\n",
       "10530    It should come as no surprise that the Jews st...\n",
       "583      Well yes that s the thing there is a differenc...\n",
       "6087     I'm from France but it 's so hard to find any ...\n",
       "                               ...                        \n",
       "5846     For God 's sake nobody cares about this childi...\n",
       "5296     Let me know if White girls making out with bla...\n",
       "5499     im from wilbraham myself i 'm looking to organ...\n",
       "867      I would definitely not recommend becoming a te...\n",
       "7418                            He should sue for racism .\n",
       "Name: text, Length: 9632, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4efd43c8-b451-49d3-abfc-bc060a372c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1458     0\n",
       "10917    0\n",
       "10530    1\n",
       "583      0\n",
       "6087     0\n",
       "        ..\n",
       "5846     0\n",
       "5296     0\n",
       "5499     0\n",
       "867      0\n",
       "7418     0\n",
       "Name: label, Length: 9632, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7110b5a0-8496-4647-9936-6a0291ce3bed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    8554\n",
       "1    1078\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e7f605a8-5fe4-4e1e-9fee-415246f05a80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(9632, 38), dtype=int32, numpy=\n",
       " array([[  101,  2061,  2111, ...,     0,     0,     0],\n",
       "        [  101,  4921,  2063, ...,     0,     0,     0],\n",
       "        [  101,  2009,  2323, ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [  101, 10047,  2013, ...,  2620,  2683,   102],\n",
       "        [  101,  1045,  2052, ...,     0,     0,     0],\n",
       "        [  101,  2002,  2323, ...,     0,     0,     0]], dtype=int32)>,\n",
       " 'token_type_ids': <tf.Tensor: shape=(9632, 38), dtype=int32, numpy=\n",
       " array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>,\n",
       " 'attention_mask': <tf.Tensor: shape=(9632, 38), dtype=int32, numpy=\n",
       " array([[1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1, ..., 1, 1, 1],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts = dict(encode_texts(train_texts.to_list()))\n",
    "test_texts = dict(encode_texts(test_texts.to_list()))\n",
    "train_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36968804-8979-421d-a798-95c09b1565f9",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4f01ad-0529-4c6b-9be8-4b55d950f00a",
   "metadata": {},
   "source": [
    "Tensorflow Privacy: Set TensorFlow version to at most 2.15.0 to avoid problems with using Keras 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "4350150f-38d4-431c-873b-b30c73abbcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d4acb6-ca59-4d07-adf2-0934bc209464",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, TFBertForSequenceClassification\n",
    "\n",
    "# Download model and configuration from huggingface.co and cache.\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"google-bert/bert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd591c39-86fb-4549-8564-3f1a14578f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0039ae69-477a-4029-ba9d-610a5425de75",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca645cbf-1fa6-41a1-97a6-7113c1a121d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all layers, except for the last encoder(Classifier)\n",
    "trainable_layers = [model.classifier]\n",
    "\n",
    "for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "for layer in trainable_layers:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "5fdebfe4-2cad-4cf3-99e6-6741e2ccbc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8273d586-30bc-4ec6-b3e5-25e1b51f54e2",
   "metadata": {},
   "source": [
    "# TensorFlow Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84421a70-18ea-48ba-af59-6ab86ef4cf53",
   "metadata": {},
   "source": [
    "## DP Model with low privacy budget (ùúÄ:10.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "8ed40b57-0169-48fc-a522-c13b6e3f71b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_norm_clip = 1.5\n",
    "noise_multiplier = 0.4\n",
    "num_microbatches = 2\n",
    "learning_rate = 1e-5\n",
    "\n",
    "if batch_size % num_microbatches != 0:\n",
    "  raise ValueError('Batch size should be an integer multiple of the number of microbatches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "d9b632c6-642f-47dd-83b1-f54b8deddd1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`compute_dp_sgd_privacy` is deprecated. It does not account for doubling of sensitivity with microbatching, and assumes Poisson subsampling, which is rarely used in practice. Please use `compute_dp_sgd_privacy_statement`, which provides appropriate context for the guarantee. To compute epsilon under different assumptions than those in `compute_dp_sgd_privacy_statement`, call the `dp_accounting` libraries directly.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10.821862030774117, 2.25)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow_privacy import compute_dp_sgd_privacy\n",
    "# Compute privacy\n",
    "compute_dp_sgd_privacy(n=train_labels.shape[0],\n",
    "                      batch_size=batch_size,\n",
    "                      noise_multiplier=noise_multiplier,\n",
    "                      epochs=epochs,\n",
    "                      delta=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "97d5dc3d-1fb9-43d4-b7d8-458e0ce834be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  109482240 \n",
      "                                                                 \n",
      " dropout_227 (Dropout)       multiple                  0 (unused)\n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  1538      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109483778 (417.65 MB)\n",
      "Trainable params: 1538 (6.01 KB)\n",
      "Non-trainable params: 109482240 (417.64 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow_privacy.privacy.optimizers.dp_optimizer_keras import DPKerasSGDOptimizer\n",
    "\n",
    "# Select your differentially private optimizer\n",
    "optimizer = DPKerasSGDOptimizer(\n",
    "    l2_norm_clip=l2_norm_clip,\n",
    "    noise_multiplier=noise_multiplier,\n",
    "    num_microbatches=num_microbatches,\n",
    "    learning_rate=learning_rate)\n",
    "\n",
    "# Select your loss function\n",
    "loss = tf.keras.losses.BinaryCrossentropy(reduction=tf.losses.Reduction.NONE)\n",
    "\n",
    "# Compile your model\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db818bfa-f800-4070-8493-d3058e4158d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1078/1078 [==============================] - 222s 206ms/step - loss: 4.3888 - accuracy: 0.4870 - val_loss: 0.9811 - val_accuracy: 0.8786\n",
      "Epoch 2/3\n",
      " 641/1078 [================>.............] - ETA: 1:02 - loss: 4.2582 - accuracy: 0.5062"
     ]
    }
   ],
   "source": [
    "# Fit your model\n",
    "history = model.fit(train_texts, train_labels,\n",
    "  epochs=epochs,\n",
    "  validation_data=(test_texts, test_labels),\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b40d372-9e13-4fdb-83fa-72de1e8aa4ce",
   "metadata": {},
   "source": [
    "## classfication report: biased to majority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "1dab4607-12f3-4091-aa96-65f039486754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.76      0.82       953\n",
      "           1       0.09      0.18      0.12       118\n",
      "\n",
      "    accuracy                           0.70      1071\n",
      "   macro avg       0.48      0.47      0.47      1071\n",
      "weighted avg       0.79      0.70      0.74      1071\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# classfication report\n",
    "outputs1 = test_pred.logits\n",
    "classifications1 = np.argmax(outputs1, axis=1)\n",
    "print(classification_report(test_labels, np.array(pred_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe4464d-626b-45de-a9b9-d6326a957d48",
   "metadata": {},
   "source": [
    "## DP Model with Tighter privacy budget (ùúÄ:4.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bc101c40-02ea-44dc-88e3-41fa029c4ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig, TFBertForSequenceClassification\n",
    "\n",
    "# Download model and configuration from huggingface.co and cache.\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"google-bert/bert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8e42bfba-68c6-4aeb-9501-c06032e6845f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all layers, except for the last encoder(Classifier)\n",
    "trainable_layers = [model.classifier]\n",
    "\n",
    "for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "for layer in trainable_layers:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b02164b4-c909-41ae-bc68-8d9824106c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_norm_clip = 1\n",
    "noise_multiplier = 0.6\n",
    "num_microbatches = 32\n",
    "learning_rate = 0.25\n",
    "batch_size = 32\n",
    "epochs = 3\n",
    "\n",
    "if batch_size % num_microbatches != 0:\n",
    "  raise ValueError('Batch size should be an integer multiple of the number of microbatches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0ba4ad37-bf0e-461a-8a40-967896576a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`compute_dp_sgd_privacy` is deprecated. It does not account for doubling of sensitivity with microbatching, and assumes Poisson subsampling, which is rarely used in practice. Please use `compute_dp_sgd_privacy_statement`, which provides appropriate context for the guarantee. To compute epsilon under different assumptions than those in `compute_dp_sgd_privacy_statement`, call the `dp_accounting` libraries directly.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4.202145584797149, 4.0)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow_privacy import compute_dp_sgd_privacy\n",
    "# Compute privacy\n",
    "compute_dp_sgd_privacy(n=train_labels.shape[0],\n",
    "                      batch_size=batch_size,\n",
    "                      noise_multiplier=noise_multiplier,\n",
    "                      epochs=epochs,\n",
    "                      delta=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "19358c69-d5ae-4dfa-aca1-bc49fb138897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  109482240 \n",
      "                                                                 \n",
      " dropout_75 (Dropout)        multiple                  0 (unused)\n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  1538      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109483778 (417.65 MB)\n",
      "Trainable params: 1538 (6.01 KB)\n",
      "Non-trainable params: 109482240 (417.64 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow_privacy.privacy.optimizers.dp_optimizer_keras import DPKerasSGDOptimizer\n",
    "\n",
    "# Select your differentially private optimizer\n",
    "optimizer = DPKerasSGDOptimizer(\n",
    "    l2_norm_clip=l2_norm_clip,\n",
    "    noise_multiplier=noise_multiplier,\n",
    "    num_microbatches=num_microbatches,\n",
    "    learning_rate=learning_rate)\n",
    "\n",
    "# Select your loss function\n",
    "loss = tf.keras.losses.BinaryCrossentropy(reduction=tf.losses.Reduction.NONE)\n",
    "\n",
    "# Compile your model\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cac258e9-b751-4fcf-a0a8-b28784916484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "301/301 [==============================] - 194s 620ms/step - loss: 1.7339 - accuracy: 0.8132 - val_loss: 1.7060 - val_accuracy: 0.1681\n",
      "Epoch 2/3\n",
      "301/301 [==============================] - 184s 612ms/step - loss: 1.7347 - accuracy: 0.3605 - val_loss: 1.7057 - val_accuracy: 0.6218\n",
      "Epoch 3/3\n",
      "301/301 [==============================] - 184s 612ms/step - loss: 1.7317 - accuracy: 0.2262 - val_loss: 1.7045 - val_accuracy: 0.1148\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x2f5be0210>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit your model\n",
    "model.fit(train_texts, train_labels,\n",
    "  epochs=epochs,\n",
    "  validation_data=(test_texts, test_labels),\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb024c1c-cbf6-440f-a3a3-ad042349ff89",
   "metadata": {},
   "source": [
    "## DP Model with Strongest privacy budget (ùúÄ:0.57)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e7cdb8ae-dc55-4307-a5a9-a2dcbbe9571f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig, TFBertForSequenceClassification\n",
    "\n",
    "# Download model and configuration from huggingface.co and cache.\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"google-bert/bert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5e44a357-2a3b-4225-9129-55226eb9db6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all layers, except for the last encoder(Classifier)\n",
    "trainable_layers = [model.classifier]\n",
    "\n",
    "for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "for layer in trainable_layers:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0705c42b-4693-4d0f-af58-13052fa7156e",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_norm_clip = 1.5\n",
    "noise_multiplier = 1.3\n",
    "num_microbatches = 32\n",
    "learning_rate = 0.25\n",
    "epochs = 5\n",
    "\n",
    "if batch_size % num_microbatches != 0:\n",
    "  raise ValueError('Batch size should be an integer multiple of the number of microbatches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a88fef4f-7c6f-4846-b78d-e955329e5706",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`compute_dp_sgd_privacy` is deprecated. It does not account for doubling of sensitivity with microbatching, and assumes Poisson subsampling, which is rarely used in practice. Please use `compute_dp_sgd_privacy_statement`, which provides appropriate context for the guarantee. To compute epsilon under different assumptions than those in `compute_dp_sgd_privacy_statement`, call the `dp_accounting` libraries directly.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5784845682832211, 18.0)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow_privacy import compute_dp_sgd_privacy\n",
    "# Compute privacy\n",
    "compute_dp_sgd_privacy(n=train_labels.shape[0],\n",
    "                      batch_size=batch_size,\n",
    "                      noise_multiplier=noise_multiplier,\n",
    "                      epochs=epochs,\n",
    "                      delta=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a56efdff-9176-4c7a-9478-be803b5a3886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  109482240 \n",
      "                                                                 \n",
      " dropout_151 (Dropout)       multiple                  0 (unused)\n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  1538      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109483778 (417.65 MB)\n",
      "Trainable params: 1538 (6.01 KB)\n",
      "Non-trainable params: 109482240 (417.64 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow_privacy.privacy.optimizers.dp_optimizer_keras import DPKerasSGDOptimizer\n",
    "\n",
    "# Select your differentially private optimizer\n",
    "optimizer = DPKerasSGDOptimizer(\n",
    "    l2_norm_clip=l2_norm_clip,\n",
    "    noise_multiplier=noise_multiplier,\n",
    "    num_microbatches=num_microbatches,\n",
    "    learning_rate=learning_rate)\n",
    "\n",
    "# Select your loss function\n",
    "loss = tf.keras.losses.BinaryCrossentropy(reduction=tf.losses.Reduction.NONE)\n",
    "\n",
    "# Compile your model\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "be50b01b-cda7-4c7e-a783-54a70e6c5a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "301/301 [==============================] - 174s 577ms/step - loss: 1.7655 - accuracy: 0.2420 - val_loss: 1.7260 - val_accuracy: 0.1251\n",
      "Epoch 2/5\n",
      "301/301 [==============================] - 176s 585ms/step - loss: 1.7516 - accuracy: 0.2116 - val_loss: 1.7198 - val_accuracy: 0.1438\n",
      "Epoch 3/5\n",
      "301/301 [==============================] - 175s 581ms/step - loss: 1.7662 - accuracy: 0.3349 - val_loss: 1.7189 - val_accuracy: 0.1569\n",
      "Epoch 4/5\n",
      "301/301 [==============================] - 176s 584ms/step - loss: 1.7637 - accuracy: 0.6000 - val_loss: 1.7189 - val_accuracy: 0.8375\n",
      "Epoch 5/5\n",
      "301/301 [==============================] - 182s 605ms/step - loss: 1.7533 - accuracy: 0.8321 - val_loss: 1.7260 - val_accuracy: 0.8898\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x3511137d0>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit your model\n",
    "model.fit(train_texts, train_labels,\n",
    "  epochs=epochs,\n",
    "  validation_data=(test_texts, test_labels),\n",
    "  batch_size=batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
